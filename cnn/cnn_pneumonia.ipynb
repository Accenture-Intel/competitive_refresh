{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "212ce0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad45a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4c07a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 128 \n",
    "IMAGE_SIZE = [180, 180]\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3518f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = tf.io.gfile.glob(str('train/*/*'))\n",
    "filenames.extend(tf.io.gfile.glob(str('val/*/*')))\n",
    "\n",
    "train_filenames, val_filenames = train_test_split(filenames, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a71ea165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# pickle.dump(train_filenames, open('train_filenames.pkl', 'wb'))\n",
    "# pickle.dump(val_filenames, open('val_filenames.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0ab2f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4185, 1047)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_filenames), len(val_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7df7fdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal images count in training set: 1100\n",
      "Pneumonia images count in training set: 3085\n"
     ]
    }
   ],
   "source": [
    "COUNT_NORMAL = len([filename for filename in train_filenames if \"NORMAL\" in filename])\n",
    "print(\"Normal images count in training set: \" + str(COUNT_NORMAL))\n",
    "\n",
    "COUNT_PNEUMONIA = len([filename for filename in train_filenames if \"PNEUMONIA\" in filename])\n",
    "print(\"Pneumonia images count in training set: \" + str(COUNT_PNEUMONIA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eba92ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'train/PNEUMONIA/person58_bacteria_273.jpeg'\n",
      "b'train/PNEUMONIA/person853_bacteria_2775.jpeg'\n",
      "b'train/PNEUMONIA/person547_bacteria_2292.jpeg'\n",
      "b'train/PNEUMONIA/person1411_bacteria_3607.jpeg'\n",
      "b'val/PNEUMONIA/person1946_bacteria_4874.jpeg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-29 17:28:41.948794: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-07-29 17:28:41.948839: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-7-97): /proc/driver/nvidia/version does not exist\n",
      "2022-07-29 17:28:41.952609: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_list_ds = tf.data.Dataset.from_tensor_slices(train_filenames)\n",
    "val_list_ds = tf.data.Dataset.from_tensor_slices(val_filenames)\n",
    "\n",
    "for f in train_list_ds.take(5):\n",
    "    print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f09182d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images count: 4185\n",
      "Validating images count: 1047\n"
     ]
    }
   ],
   "source": [
    "TRAIN_IMG_COUNT = tf.data.experimental.cardinality(train_list_ds).numpy()\n",
    "print(\"Training images count: \" + str(TRAIN_IMG_COUNT))\n",
    "\n",
    "VAL_IMG_COUNT = tf.data.experimental.cardinality(val_list_ds).numpy()\n",
    "print(\"Validating images count: \" + str(VAL_IMG_COUNT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a90b85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    return parts[-2] == \"PNEUMONIA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bcc6a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_img(img):\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.image.decode_jpeg(img, channels=3)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  # resize the image to the desired size.\n",
    "  return tf.image.resize(img, IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c86d00c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3246552",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "val_ds = val_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64638d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  (180, 180, 3)\n",
      "Label:  True\n"
     ]
    }
   ],
   "source": [
    "for image, label in train_ds.take(1):\n",
    "    print(\"Image shape: \", image.numpy().shape)\n",
    "    print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "247b9bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list_ds = tf.data.Dataset.list_files(str('test/*/*'))\n",
    "TEST_IMAGE_COUNT = tf.data.experimental.cardinality(test_list_ds).numpy()\n",
    "test_ds = test_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE)\n",
    "\n",
    "TEST_IMAGE_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89696ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "        else:\n",
    "            ds = ds.cache()\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Repeat forever\n",
    "    ds = ds.repeat()\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9544b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-29 17:28:46.170604: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "train_ds = prepare_for_training(train_ds)\n",
    "val_ds = prepare_for_training(val_ds)\n",
    "\n",
    "image_batch, label_batch = next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c555db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(filters):\n",
    "    block = tf.keras.Sequential([\n",
    "        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPool2D()\n",
    "    ]\n",
    "    )\n",
    "    \n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0beb121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block(units, dropout_rate):\n",
    "    block = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    \n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "705fd89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        \n",
    "        conv_block(32),\n",
    "        conv_block(64),\n",
    "        \n",
    "        conv_block(128),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        \n",
    "        conv_block(256),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        \n",
    "        tf.keras.layers.Flatten(),\n",
    "        dense_block(512, 0.7),\n",
    "        dense_block(128, 0.5),\n",
    "        dense_block(64, 0.3),\n",
    "        \n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0efca273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.03124148])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_bias = np.log([COUNT_PNEUMONIA/COUNT_NORMAL])\n",
    "initial_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7399a06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for class 0: 1.90\n",
      "Weight for class 1: 0.68\n"
     ]
    }
   ],
   "source": [
    "weight_for_0 = (1 / COUNT_NORMAL)*(TRAIN_IMG_COUNT)/2.0 \n",
    "weight_for_1 = (1 / COUNT_PNEUMONIA)*(TRAIN_IMG_COUNT)/2.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "801d00ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "METRICS = [\n",
    "    'accuracy',\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall')\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=METRICS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61b6422f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "32/32 [==============================] - 77s 2s/step - loss: 0.5326 - accuracy: 0.7358 - precision: 0.9344 - recall: 0.6897 - val_loss: 0.6174 - val_accuracy: 0.7607 - val_precision: 0.7607 - val_recall: 1.0000\n",
      "Epoch 2/25\n",
      "32/32 [==============================] - 60s 2s/step - loss: 0.3109 - accuracy: 0.8833 - precision: 0.9742 - recall: 0.8644 - val_loss: 0.5452 - val_accuracy: 0.7656 - val_precision: 0.7656 - val_recall: 1.0000\n",
      "Epoch 3/25\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.2435 - accuracy: 0.9124 - precision: 0.9778 - recall: 0.9019 - val_loss: 0.5915 - val_accuracy: 0.7598 - val_precision: 0.7598 - val_recall: 1.0000\n",
      "Epoch 4/25\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.2059 - accuracy: 0.9290 - precision: 0.9817 - recall: 0.9210 - val_loss: 0.6621 - val_accuracy: 0.7646 - val_precision: 0.7646 - val_recall: 1.0000\n",
      "Epoch 5/25\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.1637 - accuracy: 0.9382 - precision: 0.9789 - recall: 0.9364 - val_loss: 0.7255 - val_accuracy: 0.7627 - val_precision: 0.7627 - val_recall: 1.0000\n",
      "Epoch 6/25\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.1376 - accuracy: 0.9512 - precision: 0.9838 - recall: 0.9492 - val_loss: 0.7967 - val_accuracy: 0.7607 - val_precision: 0.7607 - val_recall: 1.0000\n",
      "Epoch 7/25\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.1374 - accuracy: 0.9534 - precision: 0.9839 - recall: 0.9524 - val_loss: 0.8344 - val_accuracy: 0.7637 - val_precision: 0.7637 - val_recall: 1.0000\n",
      "Epoch 8/25\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.1115 - accuracy: 0.9607 - precision: 0.9851 - recall: 0.9613 - val_loss: 0.8783 - val_accuracy: 0.7607 - val_precision: 0.7607 - val_recall: 1.0000\n",
      "Epoch 9/25\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.1106 - accuracy: 0.9587 - precision: 0.9866 - recall: 0.9566 - val_loss: 0.8910 - val_accuracy: 0.7637 - val_precision: 0.7637 - val_recall: 1.0000\n",
      "Epoch 10/25\n",
      "32/32 [==============================] - 59s 2s/step - loss: 0.0979 - accuracy: 0.9656 - precision: 0.9866 - recall: 0.9667 - val_loss: 0.9690 - val_accuracy: 0.7607 - val_precision: 0.7607 - val_recall: 1.0000\n",
      "Epoch 11/25\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.0937 - accuracy: 0.9651 - precision: 0.9902 - recall: 0.9624 - val_loss: 1.0401 - val_accuracy: 0.7607 - val_precision: 0.7607 - val_recall: 1.0000\n",
      "Epoch 12/25\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.0822 - accuracy: 0.9719 - precision: 0.9902 - recall: 0.9714 - val_loss: 1.0551 - val_accuracy: 0.7656 - val_precision: 0.7656 - val_recall: 1.0000\n",
      "Epoch 13/25\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.0770 - accuracy: 0.9736 - precision: 0.9906 - recall: 0.9735 - val_loss: 1.1143 - val_accuracy: 0.7617 - val_precision: 0.7617 - val_recall: 1.0000\n",
      "Epoch 14/25\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.0764 - accuracy: 0.9731 - precision: 0.9919 - recall: 0.9717 - val_loss: 1.1430 - val_accuracy: 0.7617 - val_precision: 0.7617 - val_recall: 1.0000\n",
      "Epoch 15/25\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.0616 - accuracy: 0.9771 - precision: 0.9919 - recall: 0.9766 - val_loss: 1.1627 - val_accuracy: 0.7598 - val_precision: 0.7598 - val_recall: 1.0000\n",
      "Epoch 16/25\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.0707 - accuracy: 0.9771 - precision: 0.9913 - recall: 0.9774 - val_loss: 1.1864 - val_accuracy: 0.7627 - val_precision: 0.7627 - val_recall: 1.0000\n",
      "Epoch 17/25\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.0594 - accuracy: 0.9778 - precision: 0.9926 - recall: 0.9773 - val_loss: 1.1246 - val_accuracy: 0.7627 - val_precision: 0.7627 - val_recall: 1.0000\n",
      "Epoch 18/25\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.0544 - accuracy: 0.9805 - precision: 0.9950 - recall: 0.9785 - val_loss: 1.2834 - val_accuracy: 0.7588 - val_precision: 0.7588 - val_recall: 1.0000\n",
      "Epoch 19/25\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.0392 - accuracy: 0.9871 - precision: 0.9963 - recall: 0.9861 - val_loss: 1.1024 - val_accuracy: 0.7646 - val_precision: 0.7646 - val_recall: 1.0000\n",
      "Epoch 20/25\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.0429 - accuracy: 0.9841 - precision: 0.9943 - recall: 0.9841 - val_loss: 0.6531 - val_accuracy: 0.7617 - val_precision: 0.7615 - val_recall: 1.0000\n",
      "Epoch 21/25\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.0409 - accuracy: 0.9851 - precision: 0.9956 - recall: 0.9841 - val_loss: 0.6077 - val_accuracy: 0.7754 - val_precision: 0.7721 - val_recall: 1.0000\n",
      "Epoch 22/25\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.0318 - accuracy: 0.9890 - precision: 0.9973 - recall: 0.9878 - val_loss: 0.7735 - val_accuracy: 0.7744 - val_precision: 0.7713 - val_recall: 1.0000\n",
      "Epoch 23/25\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.0344 - accuracy: 0.9875 - precision: 0.9960 - recall: 0.9871 - val_loss: 0.2173 - val_accuracy: 0.9150 - val_precision: 0.9011 - val_recall: 0.9987\n",
      "Epoch 24/25\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.0265 - accuracy: 0.9917 - precision: 0.9980 - recall: 0.9906 - val_loss: 0.6419 - val_accuracy: 0.8184 - val_precision: 0.8080 - val_recall: 1.0000\n",
      "Epoch 25/25\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.0337 - accuracy: 0.9875 - precision: 0.9963 - recall: 0.9868 - val_loss: 0.3711 - val_accuracy: 0.8848 - val_precision: 0.8685 - val_recall: 1.0000\n",
      "total training time:  1471.8910949230194\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch=TRAIN_IMG_COUNT // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=VAL_IMG_COUNT // BATCH_SIZE,\n",
    "    class_weight=class_weight,\n",
    ")\n",
    "end = time.time() - start\n",
    "print(\"total training time: \", end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40a8ec9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (mins): 24.53151824871699\n"
     ]
    }
   ],
   "source": [
    "print('Training time (mins): {}'.format(end/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77e03e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 4s 591ms/step - loss: 2.0823 - accuracy: 0.6410 - precision: 0.6352 - recall: 1.0000\n",
      "loss:  2.0822575092315674\n",
      "acc:  0.6410256624221802\n",
      "prec:  0.6351791620254517\n",
      "rec:  1.0\n",
      "total prediction time:  3.8138153553009033\n"
     ]
    }
   ],
   "source": [
    "start_eval = time.time()\n",
    "\n",
    "loss, acc, prec, rec = model.evaluate(test_ds)\n",
    "\n",
    "end_eval = time.time() - start_eval\n",
    "\n",
    "print(\"loss: \", loss)\n",
    "\n",
    "print(\"acc: \", acc)\n",
    "\n",
    "print(\"prec: \", prec)\n",
    "\n",
    "print(\"rec: \", rec)\n",
    "\n",
    "print(\"total prediction time: \", end_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p39)",
   "language": "python",
   "name": "conda_tensorflow2_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
